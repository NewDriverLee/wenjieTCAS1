main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-08-23 17:28:59 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'results': {'boolq': {'acc': 0.6584097859327217, 'acc_stderr': 0.008294560677768488}, 'arc_easy': {'acc': 0.6712962962962963, 'acc_stderr': 0.009638903167022173, 'acc_norm': 0.6182659932659933, 'acc_norm_stderr': 0.009968648851839674}, 'piqa': {'acc': 0.7589771490750816, 'acc_stderr': 0.009979042717267314, 'acc_norm': 0.7682263329706203, 'acc_norm_stderr': 0.009845143772794041}, 'lambada_openai': {'ppl': 4.037738568876969, 'ppl_stderr': 0.08695757490908669, 'acc': 0.6865903357267611, 'acc_stderr': 0.006462746304240015}, 'openbookqa': {'acc': 0.27, 'acc_stderr': 0.01987435483128748, 'acc_norm': 0.39, 'acc_norm_stderr': 0.021834685869369205}, 'arc_challenge': {'acc': 0.3293515358361775, 'acc_stderr': 0.013734057652635474, 'acc_norm': 0.35665529010238906, 'acc_norm_stderr': 0.013998056902620199}}, 'versions': {'boolq': 1, 'arc_easy': 0, 'piqa': 0, 'lambada_openai': 0, 'openbookqa': 0, 'arc_challenge': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f0ee4173b20>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-23 18:56:40 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-23 18:57:37 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu --seed 3 2023-08-23 18:58:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=3, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu --seed 3 2023-08-23 19:00:46 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=3, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.100555181503296}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu --seed 3 2023-08-23 19:05:36 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=3, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-23 19:12:52 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'ptb': 1.0626941919326782}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-23 19:20:17 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'c4': 1.008620023727417}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 10:47:11 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 10:51:04 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 10:56:02 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 10.142212867736816}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 11:27:25 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 12.459654808044434}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 11:32:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 11:34:48 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 11:40:43 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 11:43:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 11:45:22 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 11:46:53 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 11:51:01 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 11:53:14 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 11:56:52 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 11:59:38 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 12:01:31 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 12:03:15 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 14:26:11 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 14:30:51 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 14:44:37 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 14:47:02 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 14:50:26 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 14:51:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 14:56:06 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 15:05:43 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 15:08:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 15:18:43 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 15:22:35 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 15:27:07 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 15:32:52 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 15:35:25 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 15:41:23 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 15:44:35 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 15:51:14 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 16:06:37 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 16:11:15 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 16:17:51 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 16:21:10 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 16:24:04 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 16:26:57 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 16:35:13 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 16:40:27 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 16:45:41 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 16:48:11 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 16:51:03 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 16:54:48 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 17:05:18 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 17:13:01 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 17:16:35 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 17:22:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 17:25:19 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 19:22:52 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 19:26:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 19:32:27 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 19:35:35 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 19:39:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 19:47:38 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 19:49:39 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 19:55:32 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 20:05:21 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 20:08:56 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 20:14:14 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 20:28:15 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 20:36:13 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 20:47:23 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 20:54:54 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153225660324097}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 20:58:53 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153225660324097}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 21:01:56 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153225660324097}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 21:05:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153225660324097}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 21:07:15 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153225660324097}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 21:09:13 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153225660324097}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 21:15:30 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153225660324097}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-24 21:26:40 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153225660324097}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 10:28:29 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 10:31:50 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 10:36:21 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 16:11:57 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 16:15:40 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 16:25:53 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 17:44:20 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 17:48:02 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 17:54:31 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:08:17 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:17:40 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:23:04 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:30:32 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:32:20 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.1795543432235718}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:33:29 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:35:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:38:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:40:30 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:43:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:45:18 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:48:43 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:49:54 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 18:58:53 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 19:04:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.034106731414795}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 19:05:36 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.0545026063919067}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 19:08:53 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.0545026063919067}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 19:12:52 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.0545026063919067}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 19:15:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.0545026063919067}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-25 19:33:38 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 13:21:38 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 13:25:13 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 13:38:37 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 13:45:07 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 13:50:38 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 15:17:19 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 15:17:56 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 15:19:49 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 15:21:44 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 15:22:24 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 15:41:25 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 15:47:48 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 15:49:39 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 15:59:00 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 16:02:17 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 16:22:13 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 16:23:12 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 16:24:04 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 16:27:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 16:33:10 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 16:37:11 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 16:50:20 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 18:55:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 19:27:15 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 19:39:56 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 19:46:29 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 19:48:50 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 19:49:41 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 20:31:54 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks arc_challenge --multigpu 2023-08-27 20:46:41 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='arc_challenge', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 09:32:03 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 09:35:27 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 09:38:23 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 09:41:32 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 09:44:51 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 09:55:02 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 10:08:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 10:19:54 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 10:22:10 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 10:29:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 10:33:55 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 11:00:26 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 11:12:08 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 11:17:31 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 11:21:21 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 11:24:33 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 11:29:36 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 11:33:28 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 12:04:08 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 13:41:31 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 13:49:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 13:56:41 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 13:59:08 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 14:03:12 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 14:09:06 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 14:46:51 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0165414810180664}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 15:05:59 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 15:08:57 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 15:48:15 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0165414810180664}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 16:05:37 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 16:13:48 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 16:53:14 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0165414810180664}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 19:18:36 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 19:20:55 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 20:19:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0153367519378662}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 20:38:03 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=4, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.0328667163848877}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 21:07:27 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=4, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.063663125038147}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-29 22:06:50 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=4, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 5666.9775390625, 'ptb': 3298.474609375, 'c4': 2820.9794921875}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 09:43:15 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=4, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 7208.03515625, 'ptb': 4803.83544921875, 'c4': 4569.548828125}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 10:24:42 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 10.207365989685059, 'ptb': 12.382400512695312, 'c4': 11.265481948852539}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 11:08:01 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=4, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 9125.55078125, 'ptb': 5699.74755859375, 'c4': 4149.462890625}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 11:51:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=4, R2_clusters=4, R3_clusters=4, R4_clusters=4, R5_clusters=4, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 10.242886543273926, 'ptb': 12.428167343139648, 'c4': 11.308043479919434}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 12:30:25 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=2, R2_clusters=4, R3_clusters=4, R4_clusters=2, R5_clusters=2, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 10.230960845947266, 'ptb': 12.464360237121582, 'c4': 11.33278751373291}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 13:09:56 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=4, R3_clusters=4, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 10.309548377990723, 'ptb': 12.688700675964355, 'c4': 11.531975746154785}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 13:48:42 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=4, R3_clusters=4, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 10.308327674865723, 'ptb': 12.655150413513184, 'c4': 11.484060287475586}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 14:27:06 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=4, R3_clusters=4, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 10.758665084838867, 'ptb': 13.83576774597168, 'c4': 11.982874870300293}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 15:05:31 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=2, R3_clusters=2, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 10.404623985290527, 'ptb': 12.751639366149902, 'c4': 11.570289611816406}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 15:40:35 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 10.343476295471191, 'ptb': 12.773242950439453, 'c4': 11.567620277404785}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 16:17:34 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 13784.19140625, 'ptb': 7362.5908203125, 'c4': 6036.5126953125}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 19:32:23 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 10.350045204162598, 'ptb': 12.74057388305664, 'c4': 11.540975570678711}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 20:22:29 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 20:30:30 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 20:32:27 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 20:34:12 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 20:35:41 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 20:39:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 20:41:14 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 20:45:40 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 20:52:54 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 20:57:37 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 20:59:24 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-30 21:03:10 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-31 08:57:36 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-31 09:02:24 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-31 09:04:14 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1.015265941619873}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-31 09:06:41 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-31 09:07:49 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-31 09:11:11 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-31 09:20:49 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b') 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-31 09:44:56 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-31 09:48:22 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-08-31 10:35:06 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 1770.6494140625, 'ptb': 1368.103759765625, 'c4': 1661.4837646484375}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-09-04 18:28:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=400, topk_num_final_layer_norm=400, topk_num_fc2=1600, topk_num_out_proj_head=32, topk_num_q_proj_head=16, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373, 'ptb': 12.34054946899414, 'c4': 11.19964599609375, 'results': {'lambada_openai': {'ppl': 4.037738568876969, 'ppl_stderr': 0.08695757490908669, 'acc': 0.6865903357267611, 'acc_stderr': 0.006462746304240015}}, 'versions': {'lambada_openai': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7efac2ab1c00>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-09-04 20:26:46 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=400, topk_num_final_layer_norm=400, topk_num_fc2=1600, topk_num_out_proj_head=32, topk_num_q_proj_head=16, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 12.459654808044434, 'ptb': 15.949078559875488, 'c4': 14.021430015563965, 'results': {'lambada_openai': {'ppl': 4.681805724827609, 'ppl_stderr': 0.10305447446009187, 'acc': 0.6706772753735688, 'acc_stderr': 0.006547563491030418}, 'arc_easy': {'acc': 0.6481481481481481, 'acc_stderr': 0.009799078929868706, 'acc_norm': 0.593013468013468, 'acc_norm_stderr': 0.010080695355466606}, 'boolq': {'acc': 0.6293577981651376, 'acc_stderr': 0.008447316806409933}, 'arc_challenge': {'acc': 0.3310580204778157, 'acc_stderr': 0.01375206241981784, 'acc_norm': 0.3455631399317406, 'acc_norm_stderr': 0.013896938461145687}, 'piqa': {'acc': 0.7464635473340587, 'acc_stderr': 0.010150090834551784, 'acc_norm': 0.7540805223068553, 'acc_norm_stderr': 0.01004733186562519}, 'openbookqa': {'acc': 0.272, 'acc_stderr': 0.01992048320956608, 'acc_norm': 0.39, 'acc_norm_stderr': 0.021834685869369205}}, 'versions': {'lambada_openai': 0, 'arc_easy': 0, 'boolq': 1, 'arc_challenge': 0, 'piqa': 0, 'openbookqa': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f4adf45c820>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-09-04 23:20:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=128, topk_num_final_layer_norm=128, topk_num_fc2=512, topk_num_out_proj_head=8, topk_num_q_proj_head=8, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 11.473088264465332, 'ptb': 13.854394912719727, 'c4': 12.238856315612793, 'results': {'piqa': {'acc': 0.7410228509249184, 'acc_stderr': 0.010220966031405617, 'acc_norm': 0.750272034820457, 'acc_norm_stderr': 0.010099232969867485}, 'lambada_openai': {'ppl': 5.35940543664675, 'ppl_stderr': 0.12299790157494969, 'acc': 0.6343877352998254, 'acc_stderr': 0.006709649590864071}, 'boolq': {'acc': 0.6287461773700306, 'acc_stderr': 0.008450174658715913}, 'arc_challenge': {'acc': 0.3046075085324232, 'acc_stderr': 0.01344952210993249, 'acc_norm': 0.3378839590443686, 'acc_norm_stderr': 0.013822047922283514}, 'openbookqa': {'acc': 0.248, 'acc_stderr': 0.019332342821239103, 'acc_norm': 0.368, 'acc_norm_stderr': 0.021588982568353544}, 'arc_easy': {'acc': 0.6372053872053872, 'acc_stderr': 0.009865936757013938, 'acc_norm': 0.5622895622895623, 'acc_norm_stderr': 0.010179856486006908}}, 'versions': {'piqa': 0, 'lambada_openai': 0, 'boolq': 1, 'arc_challenge': 0, 'openbookqa': 0, 'arc_easy': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7fb18dab4760>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-09-05 11:43:56 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=256, topk_num_final_layer_norm=256, topk_num_fc2=512, topk_num_out_proj_head=16, topk_num_q_proj_head=16, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 11.453497886657715, 'ptb': 13.815364837646484, 'c4': 12.212621688842773, 'results': {'piqa': {'acc': 0.7459194776931447, 'acc_stderr': 0.010157271999135044, 'acc_norm': 0.7470076169749728, 'acc_norm_stderr': 0.010142888698862448}, 'lambada_openai': {'ppl': 5.399785622275636, 'ppl_stderr': 0.12464417269754433, 'acc': 0.6332233650300796, 'acc_stderr': 0.006714155098732948}, 'openbookqa': {'acc': 0.248, 'acc_stderr': 0.019332342821239103, 'acc_norm': 0.362, 'acc_norm_stderr': 0.021513662527582404}, 'arc_challenge': {'acc': 0.3148464163822526, 'acc_stderr': 0.01357265770308495, 'acc_norm': 0.3430034129692833, 'acc_norm_stderr': 0.013872423223718166}, 'boolq': {'acc': 0.6296636085626911, 'acc_stderr': 0.00844588243678367}, 'arc_easy': {'acc': 0.63510101010101, 'acc_stderr': 0.009878157021155647, 'acc_norm': 0.5580808080808081, 'acc_norm_stderr': 0.01019032812307177}}, 'versions': {'piqa': 0, 'lambada_openai': 0, 'openbookqa': 0, 'arc_challenge': 0, 'boolq': 1, 'arc_easy': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f5dddfb0760>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-09-05 13:48:44 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=256, topk_num_final_layer_norm=256, topk_num_fc2=512, topk_num_out_proj_head=16, topk_num_q_proj_head=16, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 10.358929634094238, 'ptb': 12.723444938659668, 'c4': 11.557210922241211, 'results': {'boolq': {'acc': 0.6477064220183486, 'acc_stderr': 0.008354760493906129}, 'lambada_openai': {'ppl': 4.372634229901094, 'ppl_stderr': 0.09711362137431306, 'acc': 0.6724238307781875, 'acc_stderr': 0.006538675291605717}, 'arc_challenge': {'acc': 0.32764505119453924, 'acc_stderr': 0.01371584794071935, 'acc_norm': 0.34215017064846415, 'acc_norm_stderr': 0.013864152159177275}, 'piqa': {'acc': 0.7606093579978237, 'acc_stderr': 0.00995588425029169, 'acc_norm': 0.7682263329706203, 'acc_norm_stderr': 0.00984514377279404}, 'arc_easy': {'acc': 0.6654040404040404, 'acc_stderr': 0.00968213772432791, 'acc_norm': 0.6144781144781145, 'acc_norm_stderr': 0.00998725000462901}, 'openbookqa': {'acc': 0.258, 'acc_stderr': 0.019586711785215837, 'acc_norm': 0.374, 'acc_norm_stderr': 0.02166071034720448}}, 'versions': {'boolq': 1, 'lambada_openai': 0, 'arc_challenge': 0, 'piqa': 0, 'arc_easy': 0, 'openbookqa': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7eff0fe05840>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 2023-09-07 00:49:44 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=4, topk_num_final_layer_norm=4, topk_num_fc2=4, topk_num_out_proj_head=4, topk_num_q_proj_head=4, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 12.853462219238281, 'ptb': 15.160701751708984, 'c4': 14.061549186706543, 'results': {'arc_easy': {'acc': 0.6414141414141414, 'acc_stderr': 0.009840882301225297, 'acc_norm': 0.5782828282828283, 'acc_norm_stderr': 0.010133255284012316}, 'piqa': {'acc': 0.7334058759521219, 'acc_stderr': 0.010316749863541367, 'acc_norm': 0.7535364526659413, 'acc_norm_stderr': 0.010054810789671811}, 'openbookqa': {'acc': 0.268, 'acc_stderr': 0.019827714859587574, 'acc_norm': 0.388, 'acc_norm_stderr': 0.021814300984787635}, 'arc_challenge': {'acc': 0.32593856655290104, 'acc_stderr': 0.013697432466693244, 'acc_norm': 0.35409556313993173, 'acc_norm_stderr': 0.013975454122756557}, 'lambada_openai': {'ppl': 4.751582484909344, 'ppl_stderr': 0.10562571797228767, 'acc': 0.6658257325829614, 'acc_stderr': 0.006571717150557815}, 'boolq': {'acc': 0.6333333333333333, 'acc_stderr': 0.008428386213506833}}, 'versions': {'arc_easy': 0, 'piqa': 0, 'openbookqa': 0, 'arc_challenge': 0, 'lambada_openai': 0, 'boolq': 1}, 'config': {'model': <models.opt.OPTClass object at 0x7f34c65608e0>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 2023-09-07 10:09:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=4, topk_num_final_layer_norm=4, topk_num_fc2=4, topk_num_out_proj_head=4, topk_num_q_proj_head=4, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a8 {'wikitext2': 10.973617553710938, 'ptb': 12.816000938415527, 'c4': 11.672652244567871, 'results': {'openbookqa': {'acc': 0.262, 'acc_stderr': 0.01968468882019472, 'acc_norm': 0.372, 'acc_norm_stderr': 0.0216371979857224}, 'piqa': {'acc': 0.7551686615886833, 'acc_stderr': 0.010032309105568793, 'acc_norm': 0.7731229597388466, 'acc_norm_stderr': 0.009771584259215173}, 'lambada_openai': {'ppl': 3.9482451897027175, 'ppl_stderr': 0.08442048032658526, 'acc': 0.6993984086939646, 'acc_stderr': 0.006388075353174958}, 'arc_challenge': {'acc': 0.3310580204778157, 'acc_stderr': 0.01375206241981784, 'acc_norm': 0.3583617747440273, 'acc_norm_stderr': 0.014012883334859868}, 'arc_easy': {'acc': 0.6687710437710438, 'acc_stderr': 0.00965764131135091, 'acc_norm': 0.617003367003367, 'acc_norm_stderr': 0.009974920384536486}, 'boolq': {'acc': 0.6810397553516819, 'acc_stderr': 0.00815167862952839}}, 'versions': {'openbookqa': 0, 'piqa': 0, 'lambada_openai': 0, 'arc_challenge': 0, 'arc_easy': 0, 'boolq': 1}, 'config': {'model': <models.opt.OPTClass object at 0x7f478b91c880>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 2023-09-07 12:11:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=4, topk_num_final_layer_norm=4, topk_num_fc2=4, topk_num_out_proj_head=4, topk_num_q_proj_head=4, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a8 {'wikitext2': 10.243887901306152, 'ptb': 12.434640884399414, 'c4': 11.308473587036133, 'results': {'arc_challenge': {'acc': 0.3319112627986348, 'acc_stderr': 0.013760988200880543, 'acc_norm': 0.35409556313993173, 'acc_norm_stderr': 0.01397545412275655}, 'boolq': {'acc': 0.6865443425076453, 'acc_stderr': 0.008113624272232308}, 'piqa': {'acc': 0.7524483133841132, 'acc_stderr': 0.010069703966857104, 'acc_norm': 0.7704026115342764, 'acc_norm_stderr': 0.00981268295081519}, 'lambada_openai': {'ppl': 3.9213327834296203, 'ppl_stderr': 0.08420080159012157, 'acc': 0.695711236173103, 'acc_stderr': 0.006410169885207216}, 'openbookqa': {'acc': 0.272, 'acc_stderr': 0.019920483209566072, 'acc_norm': 0.374, 'acc_norm_stderr': 0.02166071034720448}, 'arc_easy': {'acc': 0.672979797979798, 'acc_stderr': 0.009626235849372203, 'acc_norm': 0.6203703703703703, 'acc_norm_stderr': 0.009958037725468553}}, 'versions': {'arc_challenge': 0, 'boolq': 1, 'piqa': 0, 'lambada_openai': 0, 'openbookqa': 0, 'arc_easy': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f65522308e0>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 2023-09-07 14:15:13 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=4, topk_num_final_layer_norm=4, topk_num_fc2=4, topk_num_out_proj_head=4, topk_num_q_proj_head=4, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 11.309247970581055, 'ptb': 13.610626220703125, 'c4': 12.057546615600586, 'results': {'arc_easy': {'acc': 0.6376262626262627, 'acc_stderr': 0.009863468202583782, 'acc_norm': 0.5686026936026936, 'acc_norm_stderr': 0.010162752847747498}, 'openbookqa': {'acc': 0.266, 'acc_stderr': 0.019780559675655486, 'acc_norm': 0.376, 'acc_norm_stderr': 0.021683827539286122}, 'lambada_openai': {'ppl': 4.902618737131059, 'ppl_stderr': 0.1115582974067106, 'acc': 0.660974189792354, 'acc_stderr': 0.006595089668015789}, 'arc_challenge': {'acc': 0.3046075085324232, 'acc_stderr': 0.013449522109932492, 'acc_norm': 0.3430034129692833, 'acc_norm_stderr': 0.01387242322371817}, 'boolq': {'acc': 0.6336391437308868, 'acc_stderr': 0.008426904488635865}, 'piqa': {'acc': 0.7388465723612623, 'acc_stderr': 0.010248738649935581, 'acc_norm': 0.7578890097932536, 'acc_norm_stderr': 0.009994371269104387}}, 'versions': {'arc_easy': 0, 'openbookqa': 0, 'lambada_openai': 0, 'arc_challenge': 0, 'boolq': 1, 'piqa': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7fa73f31c880>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-09-07 16:17:57 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=40, R2_clusters=4, R3_clusters=4, R4_clusters=40, R5_clusters=40, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=4, topk_num_final_layer_norm=4, topk_num_fc2=4, topk_num_out_proj_head=4, topk_num_q_proj_head=4, group128=0, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 12.522037506103516, 'ptb': 15.864161491394043, 'c4': 13.900537490844727, 'results': {'arc_easy': {'acc': 0.6468855218855218, 'acc_stderr': 0.009807078935467613, 'acc_norm': 0.5900673400673401, 'acc_norm_stderr': 0.010091953527506248}, 'lambada_openai': {'ppl': 4.662180805248977, 'ppl_stderr': 0.10285025763875907, 'acc': 0.6675722879875801, 'acc_stderr': 0.006563112265118182}, 'piqa': {'acc': 0.7459194776931447, 'acc_stderr': 0.010157271999135044, 'acc_norm': 0.750272034820457, 'acc_norm_stderr': 0.010099232969867478}, 'arc_challenge': {'acc': 0.3148464163822526, 'acc_stderr': 0.01357265770308495, 'acc_norm': 0.3438566552901024, 'acc_norm_stderr': 0.01388064457015621}, 'boolq': {'acc': 0.6140672782874618, 'acc_stderr': 0.00851444449586334}, 'openbookqa': {'acc': 0.264, 'acc_stderr': 0.019732885585922098, 'acc_norm': 0.378, 'acc_norm_stderr': 0.021706550824518184}}, 'versions': {'arc_easy': 0, 'lambada_openai': 0, 'piqa': 0, 'arc_challenge': 0, 'boolq': 1, 'openbookqa': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f7f0d8208e0>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-09-07 18:11:57 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=40, R2_clusters=4, R3_clusters=4, R4_clusters=40, R5_clusters=40, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=4, topk_num_final_layer_norm=4, topk_num_fc2=4, topk_num_out_proj_head=4, topk_num_q_proj_head=4, group128=0, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a8 {'wikitext2': 10.840778350830078, 'ptb': 13.880276679992676, 'c4': 11.61905288696289, 'results': {'arc_challenge': {'acc': 0.3293515358361775, 'acc_stderr': 0.013734057652635473, 'acc_norm': 0.3515358361774744, 'acc_norm_stderr': 0.013952413699600938}, 'openbookqa': {'acc': 0.272, 'acc_stderr': 0.01992048320956607, 'acc_norm': 0.388, 'acc_norm_stderr': 0.021814300984787635}, 'arc_easy': {'acc': 0.6696127946127947, 'acc_stderr': 0.009651430216428183, 'acc_norm': 0.6144781144781145, 'acc_norm_stderr': 0.00998725000462901}, 'boolq': {'acc': 0.6706422018348623, 'acc_stderr': 0.008220002592980977}, 'lambada_openai': {'ppl': 4.091602393389318, 'ppl_stderr': 0.08833532362058195, 'acc': 0.6844556568988939, 'acc_stderr': 0.006474629636371579}, 'piqa': {'acc': 0.7562568008705114, 'acc_stderr': 0.010017199471500617, 'acc_norm': 0.7698585418933623, 'acc_norm_stderr': 0.009820832826839796}}, 'versions': {'arc_challenge': 0, 'openbookqa': 0, 'arc_easy': 0, 'boolq': 1, 'lambada_openai': 0, 'piqa': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f065ff7c8e0>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 2023-09-07 19:42:35 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=4, topk_num_final_layer_norm=4, topk_num_fc2=4, topk_num_out_proj_head=4, topk_num_q_proj_head=4, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a8 {'wikitext2': 10.201030731201172, 'ptb': 12.457328796386719, 'c4': 11.30579948425293, 'results': {'arc_challenge': {'acc': 0.3267918088737201, 'acc_stderr': 0.013706665975587336, 'acc_norm': 0.34897610921501704, 'acc_norm_stderr': 0.013928933461382496}, 'piqa': {'acc': 0.7557127312295974, 'acc_stderr': 0.010024765172284237, 'acc_norm': 0.7698585418933623, 'acc_norm_stderr': 0.009820832826839798}, 'openbookqa': {'acc': 0.284, 'acc_stderr': 0.020186703693570847, 'acc_norm': 0.394, 'acc_norm_stderr': 0.021874299301689257}, 'lambada_openai': {'ppl': 3.9142689133866524, 'ppl_stderr': 0.08373854526160607, 'acc': 0.6997865321172133, 'acc_stderr': 0.006385721127153465}, 'arc_easy': {'acc': 0.6628787878787878, 'acc_stderr': 0.009700146509130076, 'acc_norm': 0.6132154882154882, 'acc_norm_stderr': 0.009993308355370965}, 'boolq': {'acc': 0.6688073394495413, 'acc_stderr': 0.008231583858517825}}, 'versions': {'arc_challenge': 0, 'piqa': 0, 'openbookqa': 0, 'lambada_openai': 0, 'arc_easy': 0, 'boolq': 1}, 'config': {'model': <models.opt.OPTClass object at 0x7f06699e5a50>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 2023-09-07 21:39:20 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=4, topk_num_final_layer_norm=4, topk_num_fc2=4, topk_num_out_proj_head=4, topk_num_q_proj_head=4, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 14.267936706542969, 'ptb': 15.945619583129883, 'c4': 15.00345516204834, 'results': {'openbookqa': {'acc': 0.252, 'acc_stderr': 0.019435727282249526, 'acc_norm': 0.362, 'acc_norm_stderr': 0.021513662527582404}, 'lambada_openai': {'ppl': 5.150067967220006, 'ppl_stderr': 0.11627656483316463, 'acc': 0.6516592276343878, 'acc_stderr': 0.006637805195772809}, 'boolq': {'acc': 0.6431192660550459, 'acc_stderr': 0.008379147807636302}, 'piqa': {'acc': 0.7393906420021763, 'acc_stderr': 0.010241826155811625, 'acc_norm': 0.7480957562568009, 'acc_norm_stderr': 0.010128421335088686}, 'arc_easy': {'acc': 0.6287878787878788, 'acc_stderr': 0.009913599001845737, 'acc_norm': 0.5715488215488216, 'acc_norm_stderr': 0.010154195733990968}, 'arc_challenge': {'acc': 0.3199658703071672, 'acc_stderr': 0.013631345807016196, 'acc_norm': 0.33276450511945393, 'acc_norm_stderr': 0.013769863046192305}}, 'versions': {'openbookqa': 0, 'lambada_openai': 0, 'boolq': 1, 'piqa': 0, 'arc_easy': 0, 'arc_challenge': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7fd4b55a0820>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 2023-09-07 23:31:20 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=4, topk_num_final_layer_norm=4, topk_num_fc2=4, topk_num_out_proj_head=4, topk_num_q_proj_head=4, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a8 {'wikitext2': 11.022099494934082, 'ptb': 13.52171516418457, 'c4': 11.720081329345703, 'results': {'piqa': {'acc': 0.750272034820457, 'acc_stderr': 0.01009923296986749, 'acc_norm': 0.7595212187159956, 'acc_norm_stderr': 0.009971345364651062}, 'boolq': {'acc': 0.681651376146789, 'acc_stderr': 0.00814751532545837}, 'openbookqa': {'acc': 0.278, 'acc_stderr': 0.0200558338880709, 'acc_norm': 0.394, 'acc_norm_stderr': 0.02187429930168926}, 'arc_challenge': {'acc': 0.33361774744027306, 'acc_stderr': 0.01377868705417654, 'acc_norm': 0.35409556313993173, 'acc_norm_stderr': 0.013975454122756555}, 'lambada_openai': {'ppl': 3.9893346090811224, 'ppl_stderr': 0.08506475600900294, 'acc': 0.6959052978847273, 'acc_stderr': 0.006409019178962838}, 'arc_easy': {'acc': 0.664983164983165, 'acc_stderr': 0.009685160765932361, 'acc_norm': 0.6111111111111112, 'acc_norm_stderr': 0.010003248335313774}}, 'versions': {'piqa': 0, 'boolq': 1, 'openbookqa': 0, 'arc_challenge': 0, 'lambada_openai': 0, 'arc_easy': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f0cee8d07c0>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 2023-09-08 01:39:22 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=4, topk_num_final_layer_norm=4, topk_num_fc2=4, topk_num_out_proj_head=4, topk_num_q_proj_head=4, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 12.84566593170166, 'ptb': 15.052517890930176, 'c4': 14.060181617736816, 'results': {'lambada_openai': {'ppl': 4.768168246695573, 'ppl_stderr': 0.10587443858569315, 'acc': 0.6648554240248399, 'acc_stderr': 0.006576453874896004}, 'openbookqa': {'acc': 0.286, 'acc_stderr': 0.020229346329177524, 'acc_norm': 0.372, 'acc_norm_stderr': 0.0216371979857224}, 'arc_easy': {'acc': 0.6443602693602694, 'acc_stderr': 0.009822854395535489, 'acc_norm': 0.5778619528619529, 'acc_norm_stderr': 0.010134620524592268}, 'boolq': {'acc': 0.6293577981651376, 'acc_stderr': 0.00844731680640993}, 'arc_challenge': {'acc': 0.3148464163822526, 'acc_stderr': 0.013572657703084948, 'acc_norm': 0.34726962457337884, 'acc_norm_stderr': 0.01391303452962044}, 'piqa': {'acc': 0.749727965179543, 'acc_stderr': 0.010106561880089784, 'acc_norm': 0.750272034820457, 'acc_norm_stderr': 0.010099232969867483}}, 'versions': {'lambada_openai': 0, 'openbookqa': 0, 'arc_easy': 0, 'boolq': 1, 'arc_challenge': 0, 'piqa': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f35dd5a8880>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 2023-09-08 09:25:32 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 12.794602394104004, 'ptb': 15.012065887451172, 'c4': 13.7679443359375, 'results': {'arc_challenge': {'acc': 0.3199658703071672, 'acc_stderr': 0.013631345807016195, 'acc_norm': 0.35665529010238906, 'acc_norm_stderr': 0.013998056902620197}, 'arc_easy': {'acc': 0.63510101010101, 'acc_stderr': 0.009878157021155647, 'acc_norm': 0.5765993265993266, 'acc_norm_stderr': 0.010138671005289061}, 'lambada_openai': {'ppl': 4.754487586126387, 'ppl_stderr': 0.10522650150353946, 'acc': 0.6671841645643315, 'acc_stderr': 0.006565033230659719}, 'boolq': {'acc': 0.6324159021406728, 'acc_stderr': 0.008432809471149867}, 'piqa': {'acc': 0.7317736670293797, 'acc_stderr': 0.010336761992404483, 'acc_norm': 0.7459194776931447, 'acc_norm_stderr': 0.010157271999135051}, 'openbookqa': {'acc': 0.256, 'acc_stderr': 0.01953692357474761, 'acc_norm': 0.366, 'acc_norm_stderr': 0.021564276850201614}}, 'versions': {'arc_challenge': 0, 'arc_easy': 0, 'lambada_openai': 0, 'boolq': 1, 'piqa': 0, 'openbookqa': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f8059a1c850>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --topk_num 1 --topk_num_final_layer_norm 1 --topk_num_fc2 1 --topk_num_out_proj_head 1 --topk_num_q_proj_head 1 2023-09-08 11:45:36 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=1, topk_num_final_layer_norm=1, topk_num_fc2=1, topk_num_out_proj_head=1, topk_num_q_proj_head=1, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 13.038949012756348, 'ptb': 14.92954158782959, 'c4': 13.782395362854004, 'results': {'openbookqa': {'acc': 0.272, 'acc_stderr': 0.01992048320956607, 'acc_norm': 0.394, 'acc_norm_stderr': 0.02187429930168926}, 'arc_challenge': {'acc': 0.3165529010238908, 'acc_stderr': 0.013592431519068079, 'acc_norm': 0.3438566552901024, 'acc_norm_stderr': 0.013880644570156213}, 'arc_easy': {'acc': 0.6393097643097643, 'acc_stderr': 0.009853512108416737, 'acc_norm': 0.5694444444444444, 'acc_norm_stderr': 0.01016034539686008}, 'piqa': {'acc': 0.7464635473340587, 'acc_stderr': 0.010150090834551788, 'acc_norm': 0.750272034820457, 'acc_norm_stderr': 0.010099232969867478}, 'lambada_openai': {'ppl': 4.9195351179100335, 'ppl_stderr': 0.11043210292569777, 'acc': 0.6543760915971278, 'acc_stderr': 0.006625637528553935}, 'boolq': {'acc': 0.6357798165137615, 'acc_stderr': 0.0084164299367872}}, 'versions': {'openbookqa': 0, 'arc_challenge': 0, 'arc_easy': 0, 'piqa': 0, 'lambada_openai': 0, 'boolq': 1}, 'config': {'model': <models.opt.OPTClass object at 0x7fcca27948b0>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --topk_num 8 --topk_num_final_layer_norm 8 --topk_num_fc2 8 --topk_num_out_proj_head 8 --topk_num_q_proj_head 8 2023-09-08 13:47:55 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=8, topk_num_final_layer_norm=8, topk_num_fc2=8, topk_num_out_proj_head=8, topk_num_q_proj_head=8, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 12.620075225830078, 'ptb': 14.776759147644043, 'c4': 13.907273292541504, 'results': {'arc_easy': {'acc': 0.6439393939393939, 'acc_stderr': 0.009825454608416308, 'acc_norm': 0.5744949494949495, 'acc_norm_stderr': 0.010145271182591028}, 'lambada_openai': {'ppl': 4.732205747986024, 'ppl_stderr': 0.10518134324697738, 'acc': 0.6654376091597128, 'acc_stderr': 0.006573615585828716}, 'arc_challenge': {'acc': 0.3097269624573379, 'acc_stderr': 0.013512058415238361, 'acc_norm': 0.3464163822525597, 'acc_norm_stderr': 0.01390501118006325}, 'piqa': {'acc': 0.7459194776931447, 'acc_stderr': 0.010157271999135044, 'acc_norm': 0.7475516866158868, 'acc_norm_stderr': 0.010135665547362348}, 'openbookqa': {'acc': 0.274, 'acc_stderr': 0.019966103540279466, 'acc_norm': 0.378, 'acc_norm_stderr': 0.02170655082451819}, 'boolq': {'acc': 0.6314984709480123, 'acc_stderr': 0.008437199893502967}}, 'versions': {'arc_easy': 0, 'lambada_openai': 0, 'arc_challenge': 0, 'piqa': 0, 'openbookqa': 0, 'boolq': 1}, 'config': {'model': <models.opt.OPTClass object at 0x7f4f157008e0>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --topk_num 1 --topk_num_final_layer_norm 1 --topk_num_fc2 1 --topk_num_out_proj_head 1 --topk_num_q_proj_head 1 2023-09-08 15:50:10 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=1, topk_num_final_layer_norm=1, topk_num_fc2=1, topk_num_out_proj_head=1, topk_num_q_proj_head=1, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a8 {'wikitext2': 10.949837684631348, 'ptb': 12.829358100891113, 'c4': 11.683834075927734, 'results': {'boolq': {'acc': 0.6694189602446483, 'acc_stderr': 0.008227739156121646}, 'arc_easy': {'acc': 0.6666666666666666, 'acc_stderr': 0.009673016668133385, 'acc_norm': 0.6140572390572391, 'acc_norm_stderr': 0.009989277329503957}, 'openbookqa': {'acc': 0.284, 'acc_stderr': 0.02018670369357085, 'acc_norm': 0.392, 'acc_norm_stderr': 0.021854684955611253}, 'arc_challenge': {'acc': 0.3310580204778157, 'acc_stderr': 0.013752062419817836, 'acc_norm': 0.35409556313993173, 'acc_norm_stderr': 0.013975454122756553}, 'piqa': {'acc': 0.7557127312295974, 'acc_stderr': 0.010024765172284237, 'acc_norm': 0.7736670293797606, 'acc_norm_stderr': 0.009763294246879413}, 'lambada_openai': {'ppl': 3.9702171463229785, 'ppl_stderr': 0.08490444282972195, 'acc': 0.6918300019406172, 'acc_stderr': 0.006432902165497002}}, 'versions': {'boolq': 1, 'arc_easy': 0, 'openbookqa': 0, 'arc_challenge': 0, 'piqa': 0, 'lambada_openai': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f0fb36fc850>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --topk_num 2 --topk_num_final_layer_norm 2 --topk_num_fc2 2 --topk_num_out_proj_head 2 --topk_num_q_proj_head 2 2023-09-08 17:51:59 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a8 {'wikitext2': 10.962291717529297, 'ptb': 12.827131271362305, 'c4': 11.675124168395996, 'results': {'piqa': {'acc': 0.7568008705114254, 'acc_stderr': 0.01000961195385893, 'acc_norm': 0.7671381936887922, 'acc_norm_stderr': 0.009861236071080757}, 'boolq': {'acc': 0.6779816513761467, 'acc_stderr': 0.008172253300365221}, 'lambada_openai': {'ppl': 3.9215960964060073, 'ppl_stderr': 0.0839138570241343, 'acc': 0.6974577915777217, 'acc_stderr': 0.00639976496173549}, 'openbookqa': {'acc': 0.276, 'acc_stderr': 0.02001121929807353, 'acc_norm': 0.388, 'acc_norm_stderr': 0.021814300984787635}, 'arc_easy': {'acc': 0.6670875420875421, 'acc_stderr': 0.009669958978395328, 'acc_norm': 0.617003367003367, 'acc_norm_stderr': 0.009974920384536486}, 'arc_challenge': {'acc': 0.33276450511945393, 'acc_stderr': 0.01376986304619231, 'acc_norm': 0.35580204778157, 'acc_norm_stderr': 0.01399057113791876}}, 'versions': {'piqa': 0, 'boolq': 1, 'lambada_openai': 0, 'openbookqa': 0, 'arc_easy': 0, 'arc_challenge': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f17afb94880>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --topk_num 8 --topk_num_final_layer_norm 8 --topk_num_fc2 8 --topk_num_out_proj_head 8 --topk_num_q_proj_head 8 2023-09-08 19:53:14 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=8, topk_num_final_layer_norm=8, topk_num_fc2=8, topk_num_out_proj_head=8, topk_num_q_proj_head=8, group128=1, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a8 {'wikitext2': 10.965888977050781, 'ptb': 12.82657527923584, 'c4': 11.669246673583984, 'results': {'boolq': {'acc': 0.6593272171253822, 'acc_stderr': 0.008289183631379473}, 'arc_challenge': {'acc': 0.3267918088737201, 'acc_stderr': 0.013706665975587338, 'acc_norm': 0.35409556313993173, 'acc_norm_stderr': 0.013975454122756555}, 'openbookqa': {'acc': 0.27, 'acc_stderr': 0.01987435483128748, 'acc_norm': 0.384, 'acc_norm_stderr': 0.021772369465547194}, 'piqa': {'acc': 0.7584330794341676, 'acc_stderr': 0.009986718001804474, 'acc_norm': 0.7709466811751904, 'acc_norm_stderr': 0.009804509865175505}, 'arc_easy': {'acc': 0.6641414141414141, 'acc_stderr': 0.009691180932083505, 'acc_norm': 0.6153198653198653, 'acc_norm_stderr': 0.009983171707008997}, 'lambada_openai': {'ppl': 3.95581585744742, 'ppl_stderr': 0.08460053615384466, 'acc': 0.6974577915777217, 'acc_stderr': 0.006399764961735487}}, 'versions': {'boolq': 1, 'arc_challenge': 0, 'openbookqa': 0, 'piqa': 0, 'arc_easy': 0, 'lambada_openai': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7fb89d3007f0>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-09-15 11:18:42 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, batch_size=1, model='facebook/opt-13b') 
 w4a4 {'wikitext2': 10.12800121307373, 'ptb': 12.34054946899414, 'c4': 11.19964599609375, 'results': {'piqa': {'acc': 0.7589771490750816, 'acc_stderr': 0.009979042717267314, 'acc_norm': 0.7682263329706203, 'acc_norm_stderr': 0.009845143772794041}, 'boolq': {'acc': 0.6584097859327217, 'acc_stderr': 0.008294560677768488}, 'lambada_openai': {'ppl': 4.037738568876969, 'ppl_stderr': 0.08695757490908669, 'acc': 0.6865903357267611, 'acc_stderr': 0.006462746304240015}, 'openbookqa': {'acc': 0.27, 'acc_stderr': 0.01987435483128748, 'acc_norm': 0.39, 'acc_norm_stderr': 0.021834685869369205}, 'arc_easy': {'acc': 0.6712962962962963, 'acc_stderr': 0.009638903167022173, 'acc_norm': 0.6182659932659933, 'acc_norm_stderr': 0.009968648851839674}, 'arc_challenge': {'acc': 0.3293515358361775, 'acc_stderr': 0.013734057652635474, 'acc_norm': 0.35665529010238906, 'acc_norm_stderr': 0.013998056902620199}}, 'versions': {'piqa': 0, 'boolq': 1, 'lambada_openai': 0, 'openbookqa': 0, 'arc_easy': 0, 'arc_challenge': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f58a7411d50>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-09-15 13:32:28 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=40, R2_clusters=1, R3_clusters=1, R4_clusters=40, R5_clusters=160, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a8 {'wikitext2': 10.776090621948242, 'ptb': 13.908016204833984, 'c4': 11.603129386901855, 'results': {'boolq': {'acc': 0.6562691131498471, 'acc_stderr': 0.008306973049593467}, 'lambada_openai': {'ppl': 4.000486139959781, 'ppl_stderr': 0.0861907053425588, 'acc': 0.6873665825732583, 'acc_stderr': 0.0064583857167672745}, 'arc_easy': {'acc': 0.6742424242424242, 'acc_stderr': 0.009616642976885968, 'acc_norm': 0.6262626262626263, 'acc_norm_stderr': 0.009927267058259612}, 'arc_challenge': {'acc': 0.3293515358361775, 'acc_stderr': 0.013734057652635474, 'acc_norm': 0.35494880546075086, 'acc_norm_stderr': 0.013983036904094099}, 'openbookqa': {'acc': 0.28, 'acc_stderr': 0.020099950647503233, 'acc_norm': 0.38, 'acc_norm_stderr': 0.021728881438701702}, 'piqa': {'acc': 0.7573449401523396, 'acc_stderr': 0.010002002569708693, 'acc_norm': 0.7682263329706203, 'acc_norm_stderr': 0.009845143772794034}}, 'versions': {'boolq': 1, 'lambada_openai': 0, 'arc_easy': 0, 'arc_challenge': 0, 'openbookqa': 0, 'piqa': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f9a15830910>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-09-15 15:33:48 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=4, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=40, R2_clusters=1, R3_clusters=1, R4_clusters=40, R5_clusters=160, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, batch_size=1, model='facebook/opt-13b', weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}) 
 w4a4 {'wikitext2': 14.062615394592285, 'ptb': 16.3940486907959, 'c4': 16.034584045410156, 'results': {'lambada_openai': {'ppl': 4.9712383041392565, 'ppl_stderr': 0.11108397573850173, 'acc': 0.6493304870948962, 'acc_stderr': 0.006648045374603883}, 'arc_easy': {'acc': 0.6544612794612794, 'acc_stderr': 0.0097579487306703, 'acc_norm': 0.5989057239057239, 'acc_norm_stderr': 0.010057051106534378}, 'openbookqa': {'acc': 0.272, 'acc_stderr': 0.019920483209566072, 'acc_norm': 0.374, 'acc_norm_stderr': 0.02166071034720448}, 'arc_challenge': {'acc': 0.3165529010238908, 'acc_stderr': 0.01359243151906808, 'acc_norm': 0.35665529010238906, 'acc_norm_stderr': 0.013998056902620199}, 'boolq': {'acc': 0.6611620795107034, 'acc_stderr': 0.00827832575527374}, 'piqa': {'acc': 0.750816104461371, 'acc_stderr': 0.010091882770120216, 'acc_norm': 0.7546245919477693, 'acc_norm_stderr': 0.010039831320422384}}, 'versions': {'lambada_openai': 0, 'arc_easy': 0, 'openbookqa': 0, 'arc_challenge': 0, 'boolq': 1, 'piqa': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7faa8937c970>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-09-19 13:04:09 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=4, act_dist_plot=False, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=0, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', pack_weight=False, percdamp=0.01, reorder='12345', seed=2, tasks='lambada_openai', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, w_quantizer='gptq', wbits=4) 
 w4a4 {'wikitext2': 10.12800121307373, 'ptb': 12.34054946899414, 'c4': 11.19964599609375, 'results': {'lambada_openai': {'ppl': 4.518322142754023, 'ppl_stderr': 0.1002865671948573, 'acc': 0.6260430816999806, 'acc_stderr': 0.006741009966173402}}, 'versions': {'lambada_openai': 0}, 'config': {'model': <models.opt.OPTClass object at 0x7f8a3bd115e0>, 'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai --multigpu 2023-09-19 13:18:45 
 Namespace(R1_clusters=32, R2_clusters=4, R3_clusters=4, R4_clusters=32, R5_clusters=32, a_dynamic=False, abits=4, act_dist_plot=False, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=0, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', pack_weight=False, percdamp=0.01, reorder='12345', seed=2, tasks='lambada_openai', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, w_quantizer='gptq', wbits=4) 
 w4a4 {'wikitext2': 10.12800121307373, 'ptb': 12.34054946899414, 'c4': 11.19964599609375}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --topk_num 8 --topk_num_final_layer_norm 8 --topk_num_fc2 8 --topk_num_out_proj_head 8 --topk_num_q_proj_head 8 2023-09-20 16:35:38 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=8, topk_num_fc2=8, topk_num_final_layer_norm=8, topk_num_out_proj_head=8, topk_num_q_proj_head=8, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 10.897868156433105, 'ptb': 12.862812995910645, 'c4': 11.663193702697754}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --topk_num 8 --topk_num_final_layer_norm 8 --topk_num_fc2 8 --topk_num_out_proj_head 8 --topk_num_q_proj_head 8 2023-09-20 17:15:43 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=4, act_dist_plot=False, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=8, topk_num_fc2=8, topk_num_final_layer_norm=8, topk_num_out_proj_head=8, topk_num_q_proj_head=8, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a4 {'wikitext2': 13.309944152832031, 'ptb': 17.110862731933594, 'c4': 15.982919692993164}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --topk_num 8 --topk_num_final_layer_norm 8 --topk_num_fc2 8 --topk_num_out_proj_head 8 --topk_num_q_proj_head 8 2023-09-20 17:56:03 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=4, act_dist_plot=False, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=8, topk_num_fc2=8, topk_num_final_layer_norm=8, topk_num_out_proj_head=8, topk_num_q_proj_head=8, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a4 {'wikitext2': 12.562458038330078, 'ptb': 15.16530990600586, 'c4': 14.04538631439209}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --topk_num 8 --topk_num_final_layer_norm 8 --topk_num_fc2 8 --topk_num_out_proj_head 8 --topk_num_q_proj_head 8 2023-09-20 18:36:41 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=8, topk_num_fc2=8, topk_num_final_layer_norm=8, topk_num_out_proj_head=8, topk_num_q_proj_head=8, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 10.951364517211914, 'ptb': 12.846633911132812, 'c4': 11.646410942077637}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 512 2023-09-21 03:48:58 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=512, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 11.082852363586426, 'ptb': 13.259031295776367, 'c4': 11.709847450256348}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 256 2023-09-21 04:28:01 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=256, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 11.16401195526123, 'ptb': 13.197596549987793, 'c4': 11.741964340209961}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 64 2023-09-21 05:09:35 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=64, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 11.075277328491211, 'ptb': 13.673982620239258, 'c4': 11.756463050842285}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 32 2023-09-21 05:55:21 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=32, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 11.120798110961914, 'ptb': 13.791399002075195, 'c4': 11.81234073638916}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 16 2023-09-21 06:50:27 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=16, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 11.151790618896484, 'ptb': 13.969117164611816, 'c4': 11.894704818725586}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 8 2023-09-21 08:04:55 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=8, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 11.504504203796387, 'ptb': 14.255148887634277, 'c4': 12.161553382873535}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 512 2023-09-21 08:43:12 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=4, act_dist_plot=False, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=512, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a4 {'wikitext2': 13.84131145477295, 'ptb': 16.58441734313965, 'c4': 15.269229888916016}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 256 2023-09-21 09:21:34 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=4, act_dist_plot=False, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=256, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a4 {'wikitext2': 14.109679222106934, 'ptb': 16.253768920898438, 'c4': 15.118908882141113}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 64 2023-09-21 10:03:36 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=4, act_dist_plot=False, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=64, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a4 {'wikitext2': 13.541914939880371, 'ptb': 15.220698356628418, 'c4': 14.598930358886719}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 32 2023-09-21 10:50:01 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=4, act_dist_plot=False, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=32, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a4 {'wikitext2': 12.895763397216797, 'ptb': 15.021842956542969, 'c4': 13.863018989562988}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 16 2023-09-21 11:46:25 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=4, act_dist_plot=False, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=16, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a4 {'wikitext2': 12.582454681396484, 'ptb': 14.779324531555176, 'c4': 13.582625389099121}

main.py opt-13b --wbits 4 --abits 4 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 8 2023-09-21 13:00:56 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=4, act_dist_plot=False, act_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=8, k_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 4, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a4 {'wikitext2': 12.842887878417969, 'ptb': 15.3128023147583, 'c4': 13.472966194152832}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 512 2023-09-21 17:36:56 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=512, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 10.210785865783691, 'ptb': 12.451384544372559, 'c4': 11.308280944824219}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 256 2023-09-21 18:14:06 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=256, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 10.23724365234375, 'ptb': 12.451384544372559, 'c4': 11.312746047973633}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 64 2023-09-21 18:53:14 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=64, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 10.20508861541748, 'ptb': 12.450843811035156, 'c4': 11.302652359008789}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 32 2023-09-21 19:37:55 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=32, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 10.18155288696289, 'ptb': 12.445980072021484, 'c4': 11.314277648925781}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 16 2023-09-21 20:30:37 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=16, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 10.175518035888672, 'ptb': 12.45246410369873, 'c4': 11.32937240600586}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu --group128 1 --group_size 8 2023-09-21 21:41:14 
 Namespace(R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, a_dynamic=False, abits=8, act_dist_plot=False, act_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, batch_size=1, cache_dir='./data/opt/13b', calib_dataset='mix', disable_a_quant=False, disable_w_quant=False, eval_base_ppl=False, eval_ppl=True, group128=1, group_size=8, k_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, layer_norm_out_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, limit=-1, load='', metric='ema_minmax', model='facebook/opt-13b', multigpu=True, net='opt-13b', nsamples=128, num_fewshot=0, only_quant_kv=False, output_path='./output', p_quant_params={'n_bits': 8, 'metric': 'fix0to1'}, pack_weight=False, percdamp=0.01, q_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, reorder='12345', seed=2, tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', topk_num=2, topk_num_fc2=2, topk_num_final_layer_norm=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, v_quant_params={'n_bits': 8, 'per_channel_axes': [], 'symmetric': False, 'metric': 'ema_minmax', 'dynamic': False}, w_quantizer='gptq', wbits=4, weight_quant_params={'n_bits': 4, 'per_channel_axes': [0], 'symmetric': False, 'metric': 'minmax'}) 
 w4a8 {'wikitext2': 10.202242851257324, 'ptb': 12.494149208068848, 'c4': 11.369224548339844}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-10-30 10:37:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.127791404724121, 'ptb': 12.340014457702637, 'c4': 11.199389457702637}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-10-30 11:00:19 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.175945281982422, 'ptb': 12.492522239685059, 'c4': 11.27917766571045}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-01 19:14:27 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 11.555736541748047, 'ptb': 13.516435623168945, 'c4': 11.940279960632324}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-01 19:24:53 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 11.693747520446777, 'ptb': 13.88931655883789, 'c4': 12.094099998474121}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-01 20:05:53 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 11.719877243041992, 'ptb': 13.946703910827637, 'c4': 12.104485511779785}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-02 11:21:26 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.236886978149414, 'ptb': 12.384012222290039, 'c4': 11.22665786743164}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-02 11:37:44 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.198968887329102, 'ptb': 12.464360237121582, 'c4': 11.267803192138672}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-02 18:01:36 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.191003799438477, 'ptb': 12.495232582092285, 'c4': 11.289143562316895}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-02 19:30:39 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.214560508728027, 'ptb': 12.493606567382812, 'c4': 11.279629707336426}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-03 09:38:29 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.233174324035645, 'ptb': 12.465442657470703, 'c4': 11.27358627319336}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-03 16:20:01 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-03 16:21:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 1.015223503112793}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-05 19:10:56 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 386.4071350097656, 'ptb': 559.3767700195312, 'c4': 183.61988830566406}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-05 22:05:22 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.136978149414062, 'ptb': 12.364675521850586, 'c4': 11.20932674407959}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-06 10:03:30 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 11.094918251037598, 'ptb': 13.418816566467285, 'c4': 11.951192855834961}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-06 10:59:50 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.577028274536133, 'ptb': 13.078988075256348, 'c4': 11.562479972839355}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-06 11:22:08 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.21263599395752, 'ptb': 12.442739486694336, 'c4': 11.27829647064209}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-06 11:45:09 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.148441314697266, 'ptb': 12.365212440490723, 'c4': 11.222696304321289}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-06 15:08:49 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.510833740234375, 'ptb': 13.2073392868042, 'c4': 11.602156639099121}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-06 15:18:55 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.519562721252441, 'ptb': 12.998065948486328, 'c4': 11.529050827026367}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-06 16:28:14 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 9.967506408691406, 'ptb': 12.394229888916016, 'c4': 11.250000953674316}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-06 16:56:33 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 9.943684577941895, 'ptb': 12.530534744262695, 'c4': 11.361810684204102}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-06 19:05:51 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 9.982954025268555, 'ptb': 12.429244995117188, 'c4': 11.269049644470215}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-06 19:39:46 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.123764991760254, 'ptb': 12.375414848327637, 'c4': 11.2267427444458}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-06 20:11:18 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.144266128540039, 'ptb': 12.4033784866333, 'c4': 11.232612609863281}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 10:57:49 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.179351806640625, 'ptb': 12.4724760055542, 'c4': 11.276575088500977}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 11:13:30 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.138322830200195, 'ptb': 12.48764419555664, 'c4': 11.280405044555664}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 13:05:08 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 9.944865226745605, 'ptb': 12.47951602935791, 'c4': 11.312896728515625}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 13:06:08 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 9.942089080810547, 'ptb': 12.48818588256836, 'c4': 11.327427864074707}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 13:34:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.435223579406738, 'ptb': 12.873983383178711, 'c4': 11.43595027923584}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 14:21:57 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.07473373413086, 'ptb': 12.514230728149414, 'c4': 11.35494327545166}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 15:06:12 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.10478687286377, 'ptb': 12.363603591918945, 'c4': 11.211956977844238}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 15:35:08 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.018251419067383, 'ptb': 12.362530708312988, 'c4': 11.22029972076416}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 16:01:25 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.10105037689209, 'ptb': 12.356630325317383, 'c4': 11.208343505859375}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 16:36:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.055778503417969, 'ptb': 12.356094360351562, 'c4': 11.210929870605469}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 17:07:09 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.137826919555664, 'ptb': 12.485474586486816, 'c4': 11.317816734313965}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 19:25:36 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.177291870117188, 'ptb': 12.47626781463623, 'c4': 11.303622245788574}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 20:00:46 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.07536506652832, 'ptb': 12.355022430419922, 'c4': 11.20870590209961}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-07 20:47:03 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.0003080368042, 'ptb': 12.427088737487793, 'c4': 11.284687042236328}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-08 10:18:55 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.141080856323242, 'ptb': 12.397457122802734, 'c4': 11.242450714111328}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-08 10:53:03 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.058161735534668, 'ptb': 12.396380424499512, 'c4': 11.252017974853516}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-08 11:31:14 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.136273384094238, 'ptb': 12.384012222290039, 'c4': 11.238333702087402}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-08 12:01:20 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.091191291809082, 'ptb': 12.37917709350586, 'c4': 11.241206169128418}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-08 12:39:39 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.109297752380371, 'ptb': 12.375414848327637, 'c4': 11.23917007446289}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-08 13:26:40 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.217695236206055, 'ptb': 12.492522239685059, 'c4': 11.298212051391602}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-08 13:57:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.214630126953125, 'ptb': 12.490354537963867, 'c4': 11.29405403137207}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-08 14:28:59 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.211852073669434, 'ptb': 12.483307838439941, 'c4': 11.293686866760254}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-08 15:03:48 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.133373260498047, 'ptb': 12.382400512695312, 'c4': 11.237412452697754}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-08 19:35:25 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.111132621765137, 'ptb': 12.365751266479492, 'c4': 11.216277122497559}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-08 20:07:35 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.09105110168457, 'ptb': 12.370582580566406, 'c4': 11.218052864074707}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-08 21:07:12 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.114517211914062, 'ptb': 12.369508743286133, 'c4': 11.216511726379395}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 10:47:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.070658683776855, 'ptb': 12.372729301452637, 'c4': 11.219465255737305}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 11:19:23 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.116495132446289, 'ptb': 12.372729301452637, 'c4': 11.217924118041992}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 11:50:27 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.031396865844727, 'ptb': 12.37702465057373, 'c4': 11.230770111083984}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 12:20:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.120516777038574, 'ptb': 12.371119499206543, 'c4': 11.219229698181152}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 12:48:32 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 9.981561660766602, 'ptb': 12.408223152160645, 'c4': 11.257770538330078}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 13:21:12 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.141080856323242, 'ptb': 12.356094360351562, 'c4': 11.207145690917969}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 13:38:51 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.19015121459961, 'ptb': 12.406608581542969, 'c4': 11.24135684967041}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 14:09:01 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.03300666809082, 'ptb': 12.464900970458984, 'c4': 11.28808879852295}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 14:38:35 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.172467231750488, 'ptb': 12.436798095703125, 'c4': 11.250043869018555}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 15:07:57 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.082396507263184, 'ptb': 12.423850059509277, 'c4': 11.263161659240723}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 15:37:19 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.163671493530273, 'ptb': 12.415227890014648, 'c4': 11.24560260772705}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 16:07:01 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.1216459274292, 'ptb': 12.428706169128418, 'c4': 11.252747535705566}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 16:37:59 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.160977363586426, 'ptb': 12.43032455444336, 'c4': 11.244916915893555}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 19:07:40 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.128213882446289, 'ptb': 12.341085433959961, 'c4': 11.199495315551758}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 19:17:26 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.128213882446289, 'ptb': 12.341085433959961, 'c4': 11.199495315551758}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 19:37:17 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.128213882446289, 'ptb': 12.341085433959961, 'c4': 11.199495315551758}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 19:52:51 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.128213882446289, 'ptb': 12.341085433959961, 'c4': 11.199495315551758}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 20:02:41 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.128567695617676, 'ptb': 12.341623306274414, 'c4': 11.199859619140625}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 20:18:04 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.187308311462402, 'ptb': 12.371119499206543, 'c4': 11.393951416015625}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 20:36:31 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.62472152709961, 'ptb': 12.765481948852539, 'c4': 11.755633354187012}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 20:52:13 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.127791404724121, 'ptb': 12.34054946899414, 'c4': 11.199111938476562}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-09 21:08:39 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.12800121307373, 'ptb': 12.341623306274414, 'c4': 11.199517250061035}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 09:47:21 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.12736701965332, 'ptb': 12.341085433959961, 'c4': 11.199153900146484}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 10:02:52 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.126236915588379, 'ptb': 12.341085433959961, 'c4': 11.19936752319336}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 10:18:08 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.12503719329834, 'ptb': 12.339479446411133, 'c4': 11.199196815490723}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 10:36:12 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.123130798339844, 'ptb': 12.34054946899414, 'c4': 11.198769569396973}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 10:56:19 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.123199462890625, 'ptb': 12.34054946899414, 'c4': 11.199048042297363}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 11:12:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.123130798339844, 'ptb': 12.34054946899414, 'c4': 11.198769569396973}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 11:27:27 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.127861976623535, 'ptb': 12.344837188720703, 'c4': 11.202529907226562}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 11:44:26 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.157150268554688, 'ptb': 12.3748779296875, 'c4': 11.221713066101074}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 12:00:19 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.345353126525879, 'ptb': 12.589410781860352, 'c4': 11.317645072937012}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 14:10:23 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.128284454345703, 'ptb': 12.341623306274414, 'c4': 11.199389457702637}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 14:10:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.12793254852295, 'ptb': 12.341623306274414, 'c4': 11.199175834655762}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 14:20:03 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.127297401428223, 'ptb': 12.341085433959961, 'c4': 11.199132919311523}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 14:20:22 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.126166343688965, 'ptb': 12.341085433959961, 'c4': 11.199069023132324}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 14:29:42 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.124754905700684, 'ptb': 12.338944435119629, 'c4': 11.199687957763672}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 14:29:59 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.123412132263184, 'ptb': 12.34054946899414, 'c4': 11.19885540008545}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 14:39:35 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.12305736541748, 'ptb': 12.34054946899414, 'c4': 11.19896125793457}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 14:39:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.128144264221191, 'ptb': 12.344837188720703, 'c4': 11.20312786102295}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 14:49:38 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.157293319702148, 'ptb': 12.373804092407227, 'c4': 11.221434593200684}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 14:49:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.344921112060547, 'ptb': 12.589957237243652, 'c4': 11.31757926940918}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 14:59:57 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.186595916748047, 'ptb': 12.373266220092773, 'c4': 11.393863677978516}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 15:00:15 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.186810493469238, 'ptb': 12.370582580566406, 'c4': 11.394189834594727}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 15:10:01 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.186951637268066, 'ptb': 12.369508743286133, 'c4': 11.393473625183105}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 15:20:04 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.185600280761719, 'ptb': 12.3721923828125, 'c4': 11.393777847290039}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 15:29:51 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.184821128845215, 'ptb': 12.370582580566406, 'c4': 11.393842697143555}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 15:39:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.183328628540039, 'ptb': 12.371119499206543, 'c4': 11.393603324890137}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 15:49:28 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.182191848754883, 'ptb': 12.373266220092773, 'c4': 11.395472526550293}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 15:59:06 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.189297676086426, 'ptb': 12.37702465057373, 'c4': 11.400059700012207}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 16:10:08 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.123130798339844, 'ptb': 12.34054946899414, 'c4': 11.198769569396973}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 19:19:36 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 17.783382415771484, 'ptb': 19.63985252380371, 'c4': 15.694992065429688}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 19:31:07 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.110779762268066, 'ptb': 12.334661483764648, 'c4': 11.195972442626953}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 19:42:36 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.110779762268066, 'ptb': 12.334661483764648, 'c4': 11.195972442626953}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 19:54:10 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.11811637878418, 'ptb': 12.335196495056152, 'c4': 11.196762084960938}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 20:05:43 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.114234924316406, 'ptb': 12.335731506347656, 'c4': 11.195523262023926}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-10 20:17:09 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.113248825073242, 'ptb': 12.3443021774292, 'c4': 11.200350761413574}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 10:22:39 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.141646385192871, 'ptb': 12.38293743133545, 'c4': 11.22697925567627}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 10:34:04 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.335830688476562, 'ptb': 12.578486442565918, 'c4': 11.344573974609375}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 13:16:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.109652519226074, 'ptb': 12.336801528930664, 'c4': 11.195459365844727}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 13:28:35 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.109652519226074, 'ptb': 12.336801528930664, 'c4': 11.195459365844727}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 13:42:04 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.109652519226074, 'ptb': 12.336801528930664, 'c4': 11.195459365844727}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 13:53:54 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.109652519226074, 'ptb': 12.336801528930664, 'c4': 11.195459365844727}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 14:05:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.109652519226074, 'ptb': 12.336801528930664, 'c4': 11.195459365844727}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 14:17:40 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.109652519226074, 'ptb': 12.336801528930664, 'c4': 11.195459365844727}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 14:29:38 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.14922046661377, 'ptb': 12.373804092407227, 'c4': 11.212255477905273}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 14:41:36 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.239387512207031, 'ptb': 12.384549140930176, 'c4': 11.266256332397461}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 14:54:57 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 18.587230682373047, 'ptb': 18.345327377319336, 'c4': 15.576833724975586}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 15:07:03 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.10823917388916, 'ptb': 12.335196495056152, 'c4': 11.195907592773438}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 15:19:03 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.105914115905762, 'ptb': 12.331985473632812, 'c4': 11.195138931274414}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 15:30:56 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.10633659362793, 'ptb': 12.330379486083984, 'c4': 11.196548461914062}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 15:43:11 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.121505737304688, 'ptb': 12.332520484924316, 'c4': 11.208578109741211}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 15:54:59 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 25.082263946533203, 'ptb': 15.637882232666016, 'c4': 20.3255558013916}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 17:39:45 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.119247436523438, 'ptb': 12.33305549621582, 'c4': 11.195587158203125}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 17:41:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.119247436523438, 'ptb': 12.33305549621582, 'c4': 11.195587158203125}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 18:32:33 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.119247436523438, 'ptb': 12.33305549621582, 'c4': 11.195587158203125}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 18:32:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.119247436523438, 'ptb': 12.33305549621582, 'c4': 11.195587158203125}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 18:47:46 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.119247436523438, 'ptb': 12.33305549621582, 'c4': 11.195587158203125}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 18:48:15 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.119247436523438, 'ptb': 12.33305549621582, 'c4': 11.195587158203125}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 19:02:41 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.119247436523438, 'ptb': 12.33305549621582, 'c4': 11.195587158203125}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 19:03:07 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.119247436523438, 'ptb': 12.33305549621582, 'c4': 11.195587158203125}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 19:16:54 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.153467178344727, 'ptb': 12.361993789672852, 'c4': 11.222140312194824}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 19:17:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 4999.41162109375, 'ptb': 3800.14697265625, 'c4': 3384.05859375}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 19:31:22 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.132525444030762, 'ptb': 12.335731506347656, 'c4': 11.19580078125}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 19:31:45 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.158780097961426, 'ptb': 12.338944435119629, 'c4': 11.198128700256348}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 19:44:30 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.236743927001953, 'ptb': 12.354486465454102, 'c4': 11.2027006149292}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 19:44:51 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.348672866821289, 'ptb': 12.804326057434082, 'c4': 11.223617553710938}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 19:57:01 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.11042594909668, 'ptb': 12.335196495056152, 'c4': 11.195054054260254}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 20:35:53 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.103304862976074, 'ptb': 12.366289138793945, 'c4': 11.214608192443848}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-13 21:24:40 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.103096008300781, 'ptb': 12.366826057434082, 'c4': 11.214886665344238}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 11:46:28 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.237957000732422, 'ptb': 12.437338829040527, 'c4': 11.27991008758545}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 11:58:10 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.15998649597168, 'ptb': 12.359312057495117, 'c4': 11.222118377685547}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 14:27:36 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.191073417663574, 'ptb': 12.429244995117188, 'c4': 11.236405372619629}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 14:38:07 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.216129302978516, 'ptb': 12.435179710388184, 'c4': 11.252447128295898}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 14:52:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.605175971984863, 'ptb': 13.113662719726562, 'c4': 11.579275131225586}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 15:02:00 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.426419258117676, 'ptb': 12.967635154724121, 'c4': 11.4459228515625}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 15:11:07 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.479940414428711, 'ptb': 13.139871597290039, 'c4': 11.540689468383789}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 15:21:12 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 11.931455612182617, 'ptb': 15.582322120666504, 'c4': 12.861095428466797}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 15:29:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 11.708683013916016, 'ptb': 15.357396125793457, 'c4': 12.829195976257324}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 15:38:45 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 12.572714805603027, 'ptb': 17.455923080444336, 'c4': 13.920198440551758}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 15:58:27 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.138888359069824, 'ptb': 12.358240127563477, 'c4': 11.20821475982666}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 16:07:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.151272773742676, 'ptb': 12.356630325317383, 'c4': 11.210267066955566}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 16:31:14 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.112543106079102, 'ptb': 12.363066673278809, 'c4': 11.214587211608887}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 17:07:10 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.10140323638916, 'ptb': 12.3721923828125, 'c4': 11.21413803100586}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 20:58:35 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.10140323638916, 'ptb': 12.3721923828125, 'c4': 11.21413803100586}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 21:24:46 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.101825714111328, 'ptb': 12.37165641784668, 'c4': 11.212833404541016}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 21:27:35 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.14157772064209, 'ptb': 12.375414848327637, 'c4': 11.21157169342041}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 21:52:07 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 2960.7333984375, 'ptb': 1741.043212890625, 'c4': 594.42138671875}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 21:56:17 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.098727226257324, 'ptb': 12.364675521850586, 'c4': 11.211464881896973}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 22:39:34 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.084789276123047, 'ptb': 12.355558395385742, 'c4': 11.20855712890625}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 22:42:08 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.076701164245605, 'ptb': 12.352338790893555, 'c4': 11.212554931640625}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-14 23:05:43 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.104363441467285, 'ptb': 12.388849258422852, 'c4': 11.24382209777832}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-15 09:28:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.076701164245605, 'ptb': 12.352338790893555, 'c4': 11.212554931640625}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-15 09:29:48 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.089221000671387, 'ptb': 12.371119499206543, 'c4': 11.222825050354004}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-15 15:31:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.053393363952637, 'ptb': 12.359312057495117, 'c4': 11.216148376464844}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-15 15:32:23 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.043158531188965, 'ptb': 12.367362022399902, 'c4': 11.221925735473633}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-15 18:25:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.040987014770508, 'ptb': 12.363066673278809, 'c4': 11.218673706054688}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-15 18:26:02 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.033635139465332, 'ptb': 12.370582580566406, 'c4': 11.223896026611328}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-15 19:59:56 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.043158531188965, 'ptb': 12.36843490600586, 'c4': 11.221713066101074}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-15 20:01:43 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.043649673461914, 'ptb': 12.36843490600586, 'c4': 11.22132682800293}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-15 23:00:54 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.048205375671387, 'ptb': 12.358240127563477, 'c4': 11.218544960021973}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-15 23:01:28 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.066584587097168, 'ptb': 12.363603591918945, 'c4': 11.217860221862793}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-15 23:37:01 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.044280052185059, 'ptb': 12.363603591918945, 'c4': 11.222097396850586}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-15 23:37:18 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.047574996948242, 'ptb': 12.364139556884766, 'c4': 11.219979286193848}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 10:18:23 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.053532600402832, 'ptb': 12.359312057495117, 'c4': 11.216405868530273}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 11:03:52 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.056408882141113, 'ptb': 12.355022430419922, 'c4': 11.215614318847656}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 11:37:52 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.057108879089355, 'ptb': 12.359312057495117, 'c4': 11.217411041259766}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 11:38:31 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.059564590454102, 'ptb': 12.354486465454102, 'c4': 11.21520709991455}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 12:26:00 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.055216789245605, 'ptb': 12.359848022460938, 'c4': 11.215614318847656}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 12:26:10 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.053954124450684, 'ptb': 12.352874755859375, 'c4': 11.215699195861816}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 15:10:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.060689926147461, 'ptb': 12.35126781463623, 'c4': 11.216791152954102}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 15:44:42 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.069815635681152, 'ptb': 12.360920906066895, 'c4': 11.21379566192627}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 15:45:29 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.09555721282959, 'ptb': 12.366289138793945, 'c4': 11.215292930603027}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 18:14:02 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.088095664978027, 'ptb': 12.358240127563477, 'c4': 11.232269287109375}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 18:14:59 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.092390060424805, 'ptb': 12.353949546813965, 'c4': 11.232248306274414}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 18:47:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.090276718139648, 'ptb': 12.354486465454102, 'c4': 11.231476783752441}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 18:48:05 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.063776969909668, 'ptb': 12.361456871032715, 'c4': 11.2190580368042}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 19:47:55 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.062512397766113, 'ptb': 12.359312057495117, 'c4': 11.219016075134277}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 19:48:31 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.066022872924805, 'ptb': 12.358776092529297, 'c4': 11.217390060424805}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 20:20:50 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.064550399780273, 'ptb': 12.349123001098633, 'c4': 11.2169828414917}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 20:21:54 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06995677947998, 'ptb': 12.358776092529297, 'c4': 11.217560768127441}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 22:00:42 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.089290618896484, 'ptb': 12.366289138793945, 'c4': 11.217839241027832}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 22:02:03 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.075226783752441, 'ptb': 12.364675521850586, 'c4': 11.217689514160156}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 22:35:32 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06995677947998, 'ptb': 12.358776092529297, 'c4': 11.217560768127441}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 22:36:50 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06995677947998, 'ptb': 12.358776092529297, 'c4': 11.217560768127441}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 23:09:15 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06995677947998, 'ptb': 12.358776092529297, 'c4': 11.217560768127441}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 23:10:28 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06995677947998, 'ptb': 12.358776092529297, 'c4': 11.217560768127441}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 23:50:27 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06995677947998, 'ptb': 12.358776092529297, 'c4': 11.217560768127441}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-16 23:51:50 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06995677947998, 'ptb': 12.358776092529297, 'c4': 11.217560768127441}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 00:24:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06995677947998, 'ptb': 12.358776092529297, 'c4': 11.217560768127441}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 00:24:59 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06995677947998, 'ptb': 12.358776092529297, 'c4': 11.217560768127441}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 10:55:50 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06995677947998, 'ptb': 12.358776092529297, 'c4': 11.217560768127441}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 12:34:31 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.601624488830566, 'ptb': 13.799782752990723, 'c4': 11.310307502746582}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 12:35:39 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06995677947998, 'ptb': 12.358776092529297, 'c4': 11.217560768127441}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 16:46:32 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.070727348327637, 'ptb': 12.359312057495117, 'c4': 11.2189302444458}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 16:47:08 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.354450225830078, 'ptb': 12.757173538208008, 'c4': 11.40384292602539}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 17:20:42 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.069534301757812, 'ptb': 12.364139556884766, 'c4': 11.219529151916504}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 17:21:31 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.069323539733887, 'ptb': 12.35126781463623, 'c4': 11.216747283935547}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 17:58:04 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.069534301757812, 'ptb': 12.364139556884766, 'c4': 11.219529151916504}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 17:59:28 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.069534301757812, 'ptb': 12.364139556884766, 'c4': 11.219529151916504}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 18:42:59 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.069534301757812, 'ptb': 12.364139556884766, 'c4': 11.219529151916504}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 18:43:42 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.069534301757812, 'ptb': 12.364139556884766, 'c4': 11.219529151916504}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 19:16:52 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.069534301757812, 'ptb': 12.364139556884766, 'c4': 11.219529151916504}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 19:17:39 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.069534301757812, 'ptb': 12.364139556884766, 'c4': 11.219529151916504}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 19:50:42 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.069534301757812, 'ptb': 12.364139556884766, 'c4': 11.219444274902344}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 19:51:37 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.491863250732422, 'ptb': 12.818781852722168, 'c4': 11.460952758789062}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 21:03:04 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.069815635681152, 'ptb': 12.365212440490723, 'c4': 11.219657897949219}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 21:03:55 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.067426681518555, 'ptb': 12.364675521850586, 'c4': 11.219186782836914}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 21:48:28 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.070446968078613, 'ptb': 12.363066673278809, 'c4': 11.218009948730469}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 21:49:03 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.065531730651855, 'ptb': 12.360384941101074, 'c4': 11.217667579650879}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 23:57:07 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.067288398742676, 'ptb': 12.353949546813965, 'c4': 11.218438148498535}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-17 23:57:59 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.064550399780273, 'ptb': 12.352874755859375, 'c4': 11.217795372009277}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 09:40:51 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06588363647461, 'ptb': 12.362530708312988, 'c4': 11.216961860656738}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 09:42:11 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.064477920532227, 'ptb': 12.363603591918945, 'c4': 11.2169828414917}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 10:15:33 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.063146591186523, 'ptb': 12.364675521850586, 'c4': 11.218138694763184}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 10:16:33 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.067218780517578, 'ptb': 12.3748779296875, 'c4': 11.22166919708252}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 10:50:45 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.075718879699707, 'ptb': 12.387236595153809, 'c4': 11.239041328430176}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 10:51:41 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 369.8663330078125, 'ptb': 188.41151428222656, 'c4': nan}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 13:38:21 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.065250396728516, 'ptb': 12.361456871032715, 'c4': 11.219807624816895}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 13:39:31 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.062512397766113, 'ptb': 12.364139556884766, 'c4': 11.219849586486816}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 14:16:37 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06223201751709, 'ptb': 12.377565383911133, 'c4': 11.221455574035645}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 14:17:42 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.062792778015137, 'ptb': 12.365751266479492, 'c4': 11.223466873168945}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 14:59:07 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.070799827575684, 'ptb': 12.394229888916016, 'c4': 11.23066234588623}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 15:04:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.332514762878418, 'ptb': 12.582307815551758, 'c4': 35.200958251953125}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 16:55:09 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.062792778015137, 'ptb': 12.365751266479492, 'c4': 11.223466873168945}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 18:55:30 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.062792778015137, 'ptb': 12.365751266479492, 'c4': 11.223466873168945}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 19:01:11 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 14.222723960876465, 'ptb': 20.826814651489258, 'c4': 13.056976318359375}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 19:37:27 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.066935539245605, 'ptb': 12.372729301452637, 'c4': 11.222867965698242}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 19:43:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06988525390625, 'ptb': 12.376487731933594, 'c4': 11.222076416015625}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 22:49:44 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.063846588134766, 'ptb': 12.37702465057373, 'c4': 11.22385311126709}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 22:55:22 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.062792778015137, 'ptb': 12.376487731933594, 'c4': 11.22385311126709}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-18 23:42:20 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.06363582611084, 'ptb': 12.373804092407227, 'c4': 11.22359561920166}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 00:56:08 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.062373161315918, 'ptb': 12.376487731933594, 'c4': 11.22385311126709}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 10:23:50 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 11.874665260314941, 'ptb': 14.421931266784668, 'c4': 11.736522674560547}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 13:16:14 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.056266784667969, 'ptb': 12.364139556884766, 'c4': 11.220213890075684}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 13:23:18 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.061600685119629, 'ptb': 12.37004566192627, 'c4': 11.218287467956543}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 13:59:24 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.080286026000977, 'ptb': 12.395305633544922, 'c4': 11.223381996154785}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 14:04:56 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.169272422790527, 'ptb': 12.499032020568848, 'c4': 11.273178100585938}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 15:10:04 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.112120628356934, 'ptb': 12.408761978149414, 'c4': 11.238441467285156}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 15:16:19 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.081764221191406, 'ptb': 12.37917709350586, 'c4': 11.226165771484375}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 16:32:22 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.059494972229004, 'ptb': 12.364139556884766, 'c4': 11.22263240814209}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 18:03:06 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.126731872558594, 'ptb': 12.428706169128418, 'c4': 11.249786376953125}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 18:08:04 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.139171600341797, 'ptb': 12.455166816711426, 'c4': 11.254142761230469}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 18:36:52 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.177291870117188, 'ptb': 12.485474586486816, 'c4': 11.271349906921387}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 19:16:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.39802074432373, 'ptb': 12.963133811950684, 'c4': 11.4480619430542}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-19 19:20:54 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.423583984375, 'ptb': 13.037049293518066, 'c4': 11.471253395080566}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-20 09:17:46 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 1.0153934955596924}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-20 11:08:07 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.059284210205078, 'ptb': 12.36843490600586, 'c4': 11.220407485961914}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-20 18:25:46 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.079866409301758, 'ptb': 12.404993057250977, 'c4': 11.232312202453613}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-20 22:13:49 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.100066184997559, 'ptb': 12.397994995117188, 'c4': 11.224324226379395}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-20 23:03:43 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.103798866271973, 'ptb': 12.387774467468262, 'c4': 11.2189302444458}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-21 00:07:43 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.160693168640137, 'ptb': 12.428167343139648, 'c4': 11.240842819213867}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-22 13:03:44 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.098797798156738, 'ptb': 12.355022430419922, 'c4': 11.20744514465332}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-22 13:37:13 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.055778503417969, 'ptb': 12.356094360351562, 'c4': 11.210929870605469}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-22 13:52:43 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.10105037689209, 'ptb': 12.356630325317383, 'c4': 11.208343505859375}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-22 14:13:30 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.018251419067383, 'ptb': 12.362530708312988, 'c4': 11.22029972076416}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-22 14:28:47 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.10478687286377, 'ptb': 12.363603591918945, 'c4': 11.211956977844238}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-22 14:51:34 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 9.967506408691406, 'ptb': 12.394229888916016, 'c4': 11.250000953674316}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-22 15:07:14 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.123764991760254, 'ptb': 12.375414848327637, 'c4': 11.2267427444458}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-22 15:28:50 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.144266128540039, 'ptb': 12.4033784866333, 'c4': 11.232612609863281}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-22 15:47:24 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 9.943684577941895, 'ptb': 12.530534744262695, 'c4': 11.361810684204102}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-22 16:07:23 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.175945281982422, 'ptb': 12.492522239685059, 'c4': 11.27917766571045}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-22 16:32:13 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.166790962219238, 'ptb': 12.471393585205078, 'c4': 11.275672912597656}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-22 16:54:23 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.236886978149414, 'ptb': 12.384012222290039, 'c4': 11.22665786743164}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-23 11:27:32 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.188586235046387, 'ptb': 12.414151191711426, 'c4': 11.237283706665039}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-23 12:07:27 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.122634887695312, 'ptb': 12.371119499206543, 'c4': 11.217667579650879}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-23 15:17:43 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.1925687789917, 'ptb': 12.415227890014648, 'c4': 11.235761642456055}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-24 12:46:17 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.145467758178711, 'ptb': 12.382400512695312, 'c4': 11.224987030029297}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-24 13:45:12 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.189723014831543, 'ptb': 12.409299850463867, 'c4': 11.240863800048828}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-24 14:23:58 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.119105339050293, 'ptb': 12.374341011047363, 'c4': 11.219444274902344}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-24 14:39:01 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.146177291870117, 'ptb': 12.386699676513672, 'c4': 11.222996711730957}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-24 18:19:33 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.12503719329834, 'ptb': 12.378103256225586, 'c4': 11.22092056274414}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-24 22:30:44 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.152617454528809, 'ptb': 12.416844367980957, 'c4': 11.235547065734863}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-26 13:39:06 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.229463577270508, 'ptb': 12.393692016601562, 'c4': 11.227128982543945}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-26 20:55:33 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.143203735351562, 'ptb': 12.502286911010742, 'c4': 11.284385681152344}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-27 09:05:28 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.175448417663574, 'ptb': 12.477892875671387, 'c4': 11.280942916870117}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-27 09:38:16 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.135140419006348, 'ptb': 12.49686050415039, 'c4': 11.301078796386719}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-27 13:39:50 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 10.086758613586426, 'ptb': 12.463278770446777, 'c4': 11.282212257385254}

main.py opt-13b --wbits 4 --abits 8 --eval_ppl --tasks lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq --multigpu 2023-11-27 16:56:19 
 Namespace(net='opt-13b', cache_dir='./data/opt/13b', calib_dataset='mix', nsamples=128, percdamp=0.01, seed=2, metric='ema_minmax', tasks='lambada_openai,piqa,arc_easy,arc_challenge,openbookqa,boolq', eval_ppl=True, num_fewshot=0, output_path='./output', wbits=4, abits=8, load='', disable_w_quant=False, disable_a_quant=False, R1_clusters=1, R2_clusters=1, R3_clusters=1, R4_clusters=1, R5_clusters=1, reorder='12345', w_quantizer='gptq', limit=-1, a_dynamic=False, eval_base_ppl=False, act_dist_plot=False, only_quant_kv=False, pack_weight=False, multigpu=True, topk_num=2, topk_num_final_layer_norm=2, topk_num_fc2=2, topk_num_out_proj_head=2, topk_num_q_proj_head=2, group128=0, group_size=64, batch_size=1, model='facebook/opt-13b') 
 w4a8 {'wikitext2': 11.555736541748047, 'ptb': 13.516435623168945, 'c4': 11.940279960632324}

